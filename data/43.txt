MIT警示“深度学习过度依赖算力”，研究三年算法不如用10倍GPU
目前深度学习的繁荣过度依赖算力的提升，在后摩尔定律时代可能遭遇发展瓶颈，在算法改进上还需多多努力。
根据外媒Venturebeat报道，麻省理工学院联合安德伍德国际学院和巴西利亚大学的研究人员进行了一项“深度学习算力”的研究。
在研究中，为了了解深度学习性能与计算之间的联系，研究人员分析了Arxiv以及其他包含基准测试来源的1058篇论文。论文领域包括图像分类、目标检测、问答、命名实体识别和机器翻译等。
得出的结论是：训练模型的进步取决于算力的大幅提高，具体来说，计算能力提高10倍相当于三年的算法改进。
而这算力提高的背后，其实现目标所隐含的计算需求——硬件、环境和金钱成本将无法承受。      
为什么说“深度学习过度依赖算力”？

他们得出这个结论的根据，是在1058篇论文中所统计的两个信息：
1、在给定的深度学习模型中，单次传播（即权重调整）所需的浮点操作数。
2、硬件负担，或用于训练模型的硬件的计算能力，计算方式为处理器数量乘以计算速率和时间。（研究人员承认，尽管这是一种不精确的计算方法，但在他们分析的论文中，对这种计算方式的报告比其他基准要广泛。）
为了更清楚的说明“单次传播所需的浮点操作数”和“硬件负担”这两个指标，作者在合著的研究报告中，举了ImageNet的例子。
作者说，通过分析这些论文，目标检测、命名实体识别和机器翻译尤其显示出硬件负担的大幅增加，而结果的改善却相对较小。在流行的开源ImageNet基准测试中，计算能力贡献了图像分类准确率的43％。      
另外，即使是最乐观的计算，要降低ImageNet上的图像分类错误率，也需要进行10^5次以上的计算。
深度学习需要的硬件负担和计算次数自然涉及巨额资金花费。据Synced的一篇报告估计，华盛顿大学的Grover假新闻检测模型在大约两周的时间内训练费用为25,000美元。OpenAI花费了高达1200万美元来训练其GPT-3语言模型，而Google估计花费了6912美元来训练BERT，这是一种双向Transformer模型，重新定义了11种自然语言处理任务的SOTA。
在去年6月的马萨诸塞州大学阿默斯特分校的另一份报告中指出，训练和搜索某种模型所需的电量涉及大约626,000磅的二氧化碳排放量。这相当于美国普通汽车使用寿命内将近五倍的排放量。
当然，研究人员也同时指出，在算法水平上进行深度学习改进已经成为提升算力性能的重要方向。他们提到了硬件加速器，例如Google的TPU、FPGA和ASIC，以及通过网络压缩和加速技术来降低计算复杂性的尝试。他们还提到了神经架构搜索和元学习，这些方法使用优化来搜索在某一类问题上具有良好性能的架构。
OpenAI的一项研究表明，自2012年以来，将AI模型训练到ImageNet图像分类中相同性能所需的计算量每16个月减少一半。Google的Transformer架构超越了seq2seq，在seq2seq推出三年后，计算量减少了61倍。DeepMind的AlphaZero可以从头开始学习如何掌握国际象棋、将棋和围棋游戏，与一年前该系统的前身AlphaGoZero相比，其计算量减少了八倍。
计算能力的爆发结束了“AI的冬天”，并为各种任务的计算性能树立了新的基准。但是，深度学习对计算能力的巨大需求限制了它改善性能的程度，特别是在硬件性能改善的步伐变得缓慢的时代。研究人员说：“这些计算限制的可能影响迫使机器学习转向比深度学习更高效的技术。”