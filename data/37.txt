让模型向人类学说话，连带人类的惰性一起
雷锋网 AI 科技评论按：OpenAI 的长期目标之一是使用强化学习解决真实世界问题的时候也能保持实用性和安全性（这一点和 DeepMind 有类似之处），那么在 OpenAI 看来，使用语言的能力就是达到这个目标的关键因素之一。

另一方面，在目前的强化学习研究中大家观察到一种现象，就是用明确的规则约束、用预定义的反馈激励智能体的时候，它们经常反而会学会使用环境和规则中的漏洞，和人类本来设定的目标背道而驰。所以另一种思路是让智能体模仿人类，根据人类的偏好、把人类的一次次评价作为学习信号进行学习。此前这方面的研究主要针对简单的模拟环境（游戏或者机器人控制任务，比如之前 OpenAI 和 DeepMind 合作的 你做我评）。

向人类牙牙学语

OpenAI 这次想做一个大胆的尝试，把「使用语言的能力」和「根据人类的偏好学习」两者结合起来，尝试这种思路对于语言这种复杂的内容能否奏效——就是从结果出发，让模型学会人类觉得喜欢的表达方式；另外，这样学习到的语言的拓展和说理能力也能帮助我们探索人类语言偏好背后的缘由。

在这项研究中，OpenAI 在两个常见的语言任务上尝试了「根据人类的偏好学习」：一，在 BookCorpus 数据集上用正面情感或者客观描述词汇续写文本，即「带风格的续写」；二，在 TL;DR 和 CNN/Daily Mail 数据集上学习文本总结。这两个任务都可以看作文本补全这个大类中的任务：给定某个文本 X，让模型补充紧跟着的文本 Y。

OpenAI 的实验从含有 774M 参数的预训练 GPT-2 模型开始。他们对预训练模型进行精细调节，方式是让人类标注员从模型生成的每组四个样本中选择最好的一个（而不是传统的精细调节中以固定的、已经标注好的数据进行监督学习；这种做法实质上是对给模型的不同行为给予不同的反馈，所以实际上更像强化学习，OpenAI 也设计了相应的强化学习框架的反馈组件）。

由于 GPT-2 已经有不错的文本生成、续写能力（正如雷锋网 AI 科技评论在最开始的 GPT-2 报道文章中介绍的），所以前一个带风格的续写任务的精细调节只经过了 5000 次样本选择就达到了人们认为非常好的表现；对于后一个总结任务，模型经过 6 万组样本的精细调节以后学会了从给定的文本里重复整个句子来作为总结（不过它能够跳过作用不大的引言）。重复整个句子作为总结确实是确保准确度的一个好方法，不过出现这个结果也可能是因为这个任务的标注人员们（选择样本的人们）在偷懒；他们依赖这样简单的重复模式评价模型输出的结果好不好，结果让模型也学会了这样偷懒。

下面我们具体看看精细调节前后的模型在任务中的表现具体如何。

带风格的续写

带风格的续写又分为两个子任务，带正面情感的续写，客观描述的续写。我们各随机摘录四个例子。

带正面情感的续写

例 1

背景文本：她看起来很累，她刚刚哭过。她边上坐着的那个男人身材中等，不高不矮，头发凌乱得仿佛要有了艺术感。他穿了一件灰色的外套，但是他还戴了一条有火星人马文图案的黑色领带，让他的衣着没有那么严肃了。我认得他。

原模型续写：我刚刚来到这个城市的那一天就要抓我的人就是他。

精细调节后续写：他对着我微笑了一下，我也笑了一下。他看起来还算得上高兴，以他这个岁数来说。

例 2

背景文本：「现在告诉我是怎么回事吧。」他慢慢靠过来，用舌头在她嘴唇上绕了一圈。尝起来有糖和巧克力的味道，恰到好处。他热烈地吻她，他的舌头也温柔地钻进她的嘴里。

原模型续写：「是个水果沙拉。」「你的心情真不错，」他喃喃道。

精细调节后续写：这个吻如此令人满意、令人愉悦。她全身的每根神经都感觉到了这种满足。

例 3

背景文本：「一般我都会把一个人最喜欢的东西在他眼前摧毁」，他边说边把下巴凑向 Cowboy Binder 和 Barry Correda 两个人，「但对你们，我得想想办法了。因为你们什么都没有。你们一点钱都没有。

原模型续写：你没有老婆。你没有工作。你没有家。

精细调节后续写：你是个不错的小伙子，你也有很多潜力，但你还没准备好做一个明星。

例 4

背景文本：她想鼓足勇气和他一起去，但是她脑海中有涌现出最好不要参与的想法。「我已经把这份工作的联系人信息给他了。

原模型续写：我告诉他你会帮我的。」「我不想这么做。」她说

精细调节后续写：如果有什么变化，我们会告诉他的。」「好啊。」她对他笑着说。

客观描述的续写

例 1

背景文本：她棕色的眼睛根本没往他这边看。她的眼神直接绕过他，望向房间里面。「Jeff 哪去了？」「不知道。」这个回答终于让她看向他，但她又很快挪开，用眼神继续在房间里搜索。

原模型续写：Grant 站在那里盯着她。「你还好吗？」他问道。

精细调节后续写：她缩起了肩膀，她的脚步在地板上行进了一段然后停在他面前，她的嘴唇闭得紧紧的。

例 2

背景文本：问题来了，我要怎么才能把她杀了？如果她尖叫，别人就会知道我来了。虽然我整个人都充满了把她的喉咙一割两半的念头，但我得冷静行事。我松开刀柄，站着，望着。

原模型续写：她还是没有出声，她的眼镜看着游泳池。我得想办法让她出来。

精细调节后续写：有一只手伸出来抓住了她，把她提起来直到双脚离地。她又尖叫了起来，腿不停发抖。

例 3

背景文本：他非常有礼貌，吃早餐的时候一直没有说话而是静静听着。我决定保留对他的意见，他好像也保留了对我的意见。Jacob Greene 要更年长一些，大概五十来岁，像个圆面包一样壮。

原模型续写：他的头发剃得很短，他的胡子只有稀疏凌乱的一小撮。

精细调节后续写：他秃头，而且秃得非常厉害，他的衬衣也偏小，勒在他的肩膀上。

例 4

背景文本：他们似乎一开始就不顺利。Gizmo 只希望情况可以逐渐改善，但他也担心他们和安全部部长的关系永远没有机会变好了。

原模型续写：显然，这两个人根本就是水火不容。

精细调节后续写：安全部部长走进了会议室中，两只手紧握在背后。

为模型精细调节提供标注（四选一）的人类标注员们也对训练后的模型进行了评价。相比于仅仅完成了预训练的 GPT-2 模型，他们认为精细调节后的模型在带正面情感的续写任务中有 88% 的情况都更好，在客观描述的续写任务中则有 86% 的情况表现更好。

文本总结

后一个文本总结任务也分为了两个子任务，CNN/Daily Mail 数据集上的报道文章总结，以及 TL;DR（「太长，不看」）数据集上的 Reddit 讨论的总结。

这两个任务就更难一些了，OpenAI 的主模型训练用到了六万个四选一结果。而且他们还需要在线数据收集，也就是说随着模型的总结策略变化，有所改变之后的模型要继续用最新的策略生成新的结果供人类标注，整个过程是动态的、持续的，与强化学习类似。要采用这种方式的原因是，离线样本收集中，所有的样本都是最初的 GPT-2 模型生成的，人类标注员只能从这些质量不高的样本中选择，所以模型的改进也非常有限。

据人类标注员们评价，这次的模型也有很好的表现。不过，由于人类标注员们很喜欢其中一个「复制文本前三句话作为总结」的基准模型的结果（虽然这个模型确实能在所有基准模型里排在前三位，但还是说明标注员们在偷懒），就导致这样学习出的 GPT-2 模型也倾向于这样做。不过，如果把标准的有监督精细调节和人类在线标注精细调节相结合，模型的 ROUGE 分数就能排进前三位。

OpenAI 的研究人员们一共进行了四个模型的对比，原始预训练 GPT-2（即无精细调节）、人类标注、有监督学习、有监督学习+人类标注。对比的方面主要有新颖性（novelty）和准确性（accuracy）。

新颖性

如上面所述，人类标准训练出的模型倾向于直接从文本开头复制句子，所以这个模型的总结句子的新颖性是最低的。
